{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Cloud Pack for Watson AIOps v3.4 Lab For the complete product documentation, visit this link . Lab Overview In this Lab, you will play the role of an IT Operations expert that will configure the Cloud Pack for Watson AIOps (CP4WAIOps) to support IT Operations of a sample business application called QOTD. This application is being \"observed\" by Instana to pull application performance management metrics including topology. The application logs are being collected by an ElasticSearch (ELK) based log aggregator. These logs are consumed by the CP4WAIOps to create a Log Anomaly model. This model is used to detect application anomalies that will be shown in an incident Story. These are the different Lab steps: Configure the ELK Integration in CP4WAIOps to pull business application logs Configure the Instana Integration in CP4WAIOps to pull business application topology Update CP4WAIOps Policies to create stories Configure CP4WAIOps Log Anomaly Training Configure CP4WAIOps Log Anomaly Inference Verify CP4WAIOps incident stories We will define the terms Training and Inference later in the Lab. NOTE: The Lab has configuration parameters that you should copy/paste from the tables shown in this web page . The Lab call these as Lab Parameters Table . Make sure you copy the full parameter value and do not miss any characters. The following deployment architecture shows the different components that support the Lab: Lab Terms Definition Lets define some key CP4WAIOps concepts and terms that will help you understand the Lab: Events: An event is a record containing structured data summarizing key attributes of an occurrence on a managed entity, which might be a network resource, some part of that resource, or other key element associated with your network, services, or applications. An event may or may not indicate something anomalous and is a point-in-time, immutable statement about the managed entity in question. Alerts: Alerts are created when one or more events indicate an anomalous condition. Alerts represent an ongoing anomalous condition against a single managed entity. Unlike events, alerts might evolve over time as the condition changes. Alerts have a start and an end time. The creation and evolution of alerts are informed by events. CP4WAIOps automatically correlates alerts to determine what alerts are likely to share a common cause. This is determined based on a combination of: Scope-based correlation - Any alerts which have the same value for the resource field are correlated. Temporal correlation - The system continually analyzes past alerts to determine which alerts tend to frequently co-occur. When these alerts occur together again, they are correlated. Topological correlation - Any alerts which refer to resources close to each other (from a topology point of view) are correlated. Stories: Stories represent the context around an incident which is currently severely impacting IT Operations. This includes all alerts that are related to the incident and information about how the affected resources are related. The creation and evolution of stories are informed by alerts . Stories help build the understanding of the current situation and also can drive the remediation steps. In other words, stories represent an IT Operations incident and are categorized by priority from 1 (high) to 5 (low). Policies: Policies are rules that contain a condition and a set of actions that can be manual or automated. They can be triggered to automatically promote events to alerts, reduce noise by grouping alerts into a story, and assign runbooks to remediate alerts. Each policy has an execution priority number which determines which policy runs first. Runbooks: A runbook is a controlled set of automated and manual steps that support system and network operational processes. A runbook orchestrates all types of infrastructure elements, such as applications, network components, or servers. We can also use runbooks to document standard procedures that can be leveraged by IT operations. Actions: Actions are the collection of several predefined steps into a single automated tested entity that can be shared by multiple runbooks. Actions improve runbook efficiency by encapsulating procedures and operations. The following chart shows how all these terms are related:","title":"Welcome to the Cloud Pack for Watson AIOps v3.4 Lab"},{"location":"#welcome-to-the-cloud-pack-for-watson-aiops-v34-lab","text":"For the complete product documentation, visit this link .","title":"Welcome to the Cloud Pack for Watson AIOps v3.4 Lab"},{"location":"#lab-overview","text":"In this Lab, you will play the role of an IT Operations expert that will configure the Cloud Pack for Watson AIOps (CP4WAIOps) to support IT Operations of a sample business application called QOTD. This application is being \"observed\" by Instana to pull application performance management metrics including topology. The application logs are being collected by an ElasticSearch (ELK) based log aggregator. These logs are consumed by the CP4WAIOps to create a Log Anomaly model. This model is used to detect application anomalies that will be shown in an incident Story. These are the different Lab steps: Configure the ELK Integration in CP4WAIOps to pull business application logs Configure the Instana Integration in CP4WAIOps to pull business application topology Update CP4WAIOps Policies to create stories Configure CP4WAIOps Log Anomaly Training Configure CP4WAIOps Log Anomaly Inference Verify CP4WAIOps incident stories We will define the terms Training and Inference later in the Lab. NOTE: The Lab has configuration parameters that you should copy/paste from the tables shown in this web page . The Lab call these as Lab Parameters Table . Make sure you copy the full parameter value and do not miss any characters. The following deployment architecture shows the different components that support the Lab:","title":"Lab Overview"},{"location":"#lab-terms-definition","text":"Lets define some key CP4WAIOps concepts and terms that will help you understand the Lab: Events: An event is a record containing structured data summarizing key attributes of an occurrence on a managed entity, which might be a network resource, some part of that resource, or other key element associated with your network, services, or applications. An event may or may not indicate something anomalous and is a point-in-time, immutable statement about the managed entity in question. Alerts: Alerts are created when one or more events indicate an anomalous condition. Alerts represent an ongoing anomalous condition against a single managed entity. Unlike events, alerts might evolve over time as the condition changes. Alerts have a start and an end time. The creation and evolution of alerts are informed by events. CP4WAIOps automatically correlates alerts to determine what alerts are likely to share a common cause. This is determined based on a combination of: Scope-based correlation - Any alerts which have the same value for the resource field are correlated. Temporal correlation - The system continually analyzes past alerts to determine which alerts tend to frequently co-occur. When these alerts occur together again, they are correlated. Topological correlation - Any alerts which refer to resources close to each other (from a topology point of view) are correlated. Stories: Stories represent the context around an incident which is currently severely impacting IT Operations. This includes all alerts that are related to the incident and information about how the affected resources are related. The creation and evolution of stories are informed by alerts . Stories help build the understanding of the current situation and also can drive the remediation steps. In other words, stories represent an IT Operations incident and are categorized by priority from 1 (high) to 5 (low). Policies: Policies are rules that contain a condition and a set of actions that can be manual or automated. They can be triggered to automatically promote events to alerts, reduce noise by grouping alerts into a story, and assign runbooks to remediate alerts. Each policy has an execution priority number which determines which policy runs first. Runbooks: A runbook is a controlled set of automated and manual steps that support system and network operational processes. A runbook orchestrates all types of infrastructure elements, such as applications, network components, or servers. We can also use runbooks to document standard procedures that can be leveraged by IT operations. Actions: Actions are the collection of several predefined steps into a single automated tested entity that can be shared by multiple runbooks. Actions improve runbook efficiency by encapsulating procedures and operations. The following chart shows how all these terms are related:","title":"Lab Terms Definition"},{"location":"about/","text":"Lab: Cloud Pak for Watson AIOps v3.4 - Log Anomaly + Instana topology Author: Jorge Gonzalez Orozco Date: August 2022 \u00a9 Copyright IBM Corp. 2022","title":"About"},{"location":"inference/","text":"Log Anomaly Inference Recap Lets recap what we have done so far: We have defined an integration to a Log Aggregator (EFK) that received historical application logs for a \"normal\" day of operations of a sample application called qotd . We have defined an integration to Instana which is observing the environment where the qotd application is running and we have pulled every resource topology information related to this application into CP4WAIOps. We have enabled one CP4WAIOps policy to create Stories. We have created a log anomaly model by training on one day of \"normal\" historical log data from the qotd application. We have deployed this model so we can actually test it with real time log data. Start Log Anomaly Inference on real-time Logs Now that our log anomaly trained model is successfully deployed, its ready to start doing log anomaly inference. That is, \"scan\" logs coming in real time from the sample application qotd and create alerts when the real time logs deviate from what is considered \"normal\" in the trained model. The final important point here is that, during the Lab execution, we are injecting real time anomalies into the sample application qotd, for example we are shutting down API services, increasing service responce latency, etc. This concept is similar to what the IT Ops team at Netflix call Chaos Monkey . These anomalies will be detected by the CP4WAIOps and you will see the new alerts and stories created. We are going to modify our integration with the log aggregator to start consuming real-time logs instead historical logs as we did previously. From the Home page, under Overview clik on Data and tool connections on the left side of the page. Click on the ELK connection type. Click on the EFK for QOTD connection definition. Click the Next button twice to arrive to the AI training and log data page. In the AI training and log data page: Make sure the Data flow is set to On (green). Choose Live data for continuous AI training and anomaly detection as the option for training mode. The AI training and log data page should look like the screenshot below: Finally click on the Save button. After saving the configuration, make sure the ELK integration page shows the Data Flow Status as Running as shown below. Note that it may take 1-2 minutes to show this. Verify Incident Story After deploying the log anomaly model and enabling real-time log consumption, we need to wait a few minutes to see the new alerts and stories being created. From the Home page, under Overview clik on Stories and alerts on the left side of the page. On the Stories tab, make sure you see one or more stories created, as shown below. Confirm that the story has been recently created. If you don't see any stories, just wait a few minutes. As we have mentioned before during the definition of the terms, from an IT Operations point of view, we have Events where some will become Alerts that get correlated and will trigger a policy that will trigger the creation of Stories . Alerts tab: Click on the Alerts tab. Here you will see one or more alerts, as shown below: These are the Log Anomaly alerts that were created by the Log Anomaly model we just deployed. They were created because the real-time logs shown patterns that are not considered \"normal\" by the trained model. Click on the first alert under the Summary column to explore the alert details on the right side of the page. On the Alert details under Properties review the values of the different properties. Look at the value of the property eventCount , this is the number of events that were deduplicated into a single Alert. Stories tab: Click back on the Stories tab, these are the stories that were created based on the alerts that you just saw before. In a real production environment, a story can group and correlate different but related alerts into a single story page. For example, lets imagine that the storage of a key application database goes down. In this scenario, we could have alerts from the database monitoring platform, log anomaly alerts from the APIs that use the database and web page response alerts that shows database data. All these three alerts will be combined into a single story in order to help the IT Operations personnel find the root cause of the incident. Click on the story Description to look at the story details. You will see the story page. Make sure to follow the pop-up page tour. Story description: On the left side, we see the Top alerts that triggered this story. These alerts are ranked by probable cause showing the top one as the most probable root cause of the overall incident. In the middle of the page, we see topology information that was provided by Instana with the problematic resource highlighted. The top right side of the page shows the StoryID, the story owner (Unassigned) and the priority. The Change story settings button on the top right allow us to change the priority, the status and assignee. The Alerts tab shows only the alerts related to this particular story. The Topology tab shows the overall topology context where this incident happen. CONGRATULATIONS, THIS IS THE END OF THE LAB","title":"Log Anomaly Inference"},{"location":"inference/#log-anomaly-inference","text":"","title":"Log Anomaly Inference"},{"location":"inference/#recap","text":"Lets recap what we have done so far: We have defined an integration to a Log Aggregator (EFK) that received historical application logs for a \"normal\" day of operations of a sample application called qotd . We have defined an integration to Instana which is observing the environment where the qotd application is running and we have pulled every resource topology information related to this application into CP4WAIOps. We have enabled one CP4WAIOps policy to create Stories. We have created a log anomaly model by training on one day of \"normal\" historical log data from the qotd application. We have deployed this model so we can actually test it with real time log data.","title":"Recap"},{"location":"inference/#start-log-anomaly-inference-on-real-time-logs","text":"Now that our log anomaly trained model is successfully deployed, its ready to start doing log anomaly inference. That is, \"scan\" logs coming in real time from the sample application qotd and create alerts when the real time logs deviate from what is considered \"normal\" in the trained model. The final important point here is that, during the Lab execution, we are injecting real time anomalies into the sample application qotd, for example we are shutting down API services, increasing service responce latency, etc. This concept is similar to what the IT Ops team at Netflix call Chaos Monkey . These anomalies will be detected by the CP4WAIOps and you will see the new alerts and stories created. We are going to modify our integration with the log aggregator to start consuming real-time logs instead historical logs as we did previously. From the Home page, under Overview clik on Data and tool connections on the left side of the page. Click on the ELK connection type. Click on the EFK for QOTD connection definition. Click the Next button twice to arrive to the AI training and log data page. In the AI training and log data page: Make sure the Data flow is set to On (green). Choose Live data for continuous AI training and anomaly detection as the option for training mode. The AI training and log data page should look like the screenshot below: Finally click on the Save button. After saving the configuration, make sure the ELK integration page shows the Data Flow Status as Running as shown below. Note that it may take 1-2 minutes to show this.","title":"Start Log Anomaly Inference on real-time Logs"},{"location":"inference/#verify-incident-story","text":"After deploying the log anomaly model and enabling real-time log consumption, we need to wait a few minutes to see the new alerts and stories being created. From the Home page, under Overview clik on Stories and alerts on the left side of the page. On the Stories tab, make sure you see one or more stories created, as shown below. Confirm that the story has been recently created. If you don't see any stories, just wait a few minutes. As we have mentioned before during the definition of the terms, from an IT Operations point of view, we have Events where some will become Alerts that get correlated and will trigger a policy that will trigger the creation of Stories . Alerts tab: Click on the Alerts tab. Here you will see one or more alerts, as shown below: These are the Log Anomaly alerts that were created by the Log Anomaly model we just deployed. They were created because the real-time logs shown patterns that are not considered \"normal\" by the trained model. Click on the first alert under the Summary column to explore the alert details on the right side of the page. On the Alert details under Properties review the values of the different properties. Look at the value of the property eventCount , this is the number of events that were deduplicated into a single Alert. Stories tab: Click back on the Stories tab, these are the stories that were created based on the alerts that you just saw before. In a real production environment, a story can group and correlate different but related alerts into a single story page. For example, lets imagine that the storage of a key application database goes down. In this scenario, we could have alerts from the database monitoring platform, log anomaly alerts from the APIs that use the database and web page response alerts that shows database data. All these three alerts will be combined into a single story in order to help the IT Operations personnel find the root cause of the incident. Click on the story Description to look at the story details. You will see the story page. Make sure to follow the pop-up page tour. Story description: On the left side, we see the Top alerts that triggered this story. These alerts are ranked by probable cause showing the top one as the most probable root cause of the overall incident. In the middle of the page, we see topology information that was provided by Instana with the problematic resource highlighted. The top right side of the page shows the StoryID, the story owner (Unassigned) and the priority. The Change story settings button on the top right allow us to change the priority, the status and assignee. The Alerts tab shows only the alerts related to this particular story. The Topology tab shows the overall topology context where this incident happen. CONGRATULATIONS, THIS IS THE END OF THE LAB","title":"Verify Incident Story"},{"location":"integration/","text":"Configuring Data Sources Connecting to the CP4WAIOps console To run this Lab, make sure that you use the Chrome or Firefox browser and that the language in your browser settings is set to English . Lets start by connecting to the CP4WAIOps console. Use the URL and login credentials given by your Lab coordinator. You will see the console Home page as shown below. Note that the Lab environments use self signed certificates. During login the browser will ask you to accept the risk. You will probably get two prompts to accept the risk. On the login page, select IBM provided credentials (admin only) . As you go to different new pages in the environment, you will see the Page Tour pop-up. Feel free to follow them. Configure the EFK Integration EFK is a variant of ELK (Elasticsearch, Logstash, and Kibana). EFK is a suite of tools combining Elasticsearch, Fluentd, and Kibana that functions as an application log aggregation tool. Note that Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. Now, to simplify the installation and configuration effort for installing the different components of the EFK stack on OpenShift, we leverage the OpenShift Logging library from OpenShift. OpenShift customers that prefer not to spend part of their budget on a commercial log aggregator such as Humio, Splunk, or LogDNA, more than likely use the OpenShift Logging library that comes out of the box. To have the CP4WAIOps collect logs from the EFK installation that leverages the OpenShift Logging library, you need to define an EFK integration. The lab will provide the values that you should use for defining the EFK integration. Note that we will configure this connection but we will leave the data flow disabled because historical log training data has already been loaded to speed up the Lab. We will enable the data flow later in the Lab during the final Log Anomaly Inference section From the Home page, under Overview clik on Data and tool connections on the left side of the page. Click on the Add connection button on the top right. On the ELK card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Complete the ELK, Add connection form, with the following values: Name : Name of the ELK integration, for example EFK for QOTD . Description : Leave it blank. ELK service URL : Get the service URL for the EFK installation from the Lab Parameters Table . Make sure to include the last * character at the end. Kibana URL : Get URL for Kibana from the Lab Parameters Table . Authentication type : Set this value to Token . Token : Get the token from the Lab Parameters Table . Certificate : Leave it blank. Filters : Don't change it. Time zone : Select GMT-4 . Kibana port : Type 443 . Base parallelism : Don't change it. Sampling rate : Don't change it. JSON processing option : Don't change it. Click on the Test connection button and confirm you get Test succeeded Click on the Next button. Field mapping : Use the mapping shown below instead of the default mapping provided on the ELK integration. Make sure you see the Valid JSON configuration message after that: { \"codec\": \"elk\", \"message_field\": \"message\", \"log_entity_types\": \"kubernetes.container_image_id, kubernetes.host, kubernetes.pod_name, kubernetes.namespace_name\", \"instance_id_field\": \"kubernetes.container_name\", \"rolling_time\": 10, \"timestamp_field\": \"@timestamp\" } Click on the Next button. Data flow : we will leave this Off (grey) as the historical log data has already been loaded to speed up the Lab. We will enable the data flow later in the Lab during the final Log Anomaly Inference section Mode : Select the Historical data for initial AI training option using the dates listed below. CP4WAIOps will ingest one day of historical application log data (stored in the log aggregator) that we know in advance that can be used as a \"reference\" for a normal day because no major IT Operations incident happen during that day. We will use this data later for Log Anomaly training. Start date: May 8, 2022 End Date: May 8, 2022 Source parallelism (1-50) : Don't change it. Click on Done . The following screenshots show the form update flow as guidance (note that config values may be different in the screenshots, follow the instructions instead) After some time, you would see the message Connection completed. IBM Cloud Pak for Watson AIOps has successfully processed your request as shown below. Configure the Instana Integration The CP4WAIOps will also consume topology information from Instana therefore we will configure this integration. Lets verify first that there is no topology data in the system. From the navigator menu, go to the Home page again, clik on Resource management under Overview on the left side of the page. On the Resource management page, make sure there are no Applications, Resource groups nor Resources defined, as shown below Now, lets define the Instana integration. From the Home page, clik on Data and tool connections under Overview on the left side of the page. Click on the Add connection button on the top right. On the Instana card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Enter the following Add connection information: Name : The display name of your connection, for example Instana connection . Description : An optional description for the connection. Leave it blank. Endpoint : Get the URL for the Instana Endpoint from the Lab Parameters Table . API Token : Get the API Token from the Lab Parameters Table . Deployment options : Select local (Note that it is possible to deploy an Instana conection gateway remotely, but in this lab we will deploy in the same local cluster). Click Next. Enter the following Collect topology data : Enable data flow : Set this toggle button to on (green). Time window (seconds): Leave as it is. This is the windowSize parameter within the Instana API. Connection Intervals (seconds): Leave as it is. This is how frequently to run the job to collect topology. Application name allowlist pattern : This allows to select from the set of applications that Instana is \"observing\" which one we will pull data from. In this Lab, we will pull topology data from a single application called qotd. Type in qotd . Import Instana application perspectives as Cloud Pak for Watson AIOps applications : Make sure this toggle button is set to on (green). This option will save us some time as we don't need to manually create in CP4WAIOps an application that group the topology resources that we pull from Instana. In CP4WAIOps, an application represents a group of resources put together. Click Next. Enter the following in Collect event data : Enable data flow : Slide this toggle button to off (grey). We will not collect event data in this Lab. Click Next. Enter the following in Collect metric data : Enable data flow : Slide this toggle button to off (grey). We will not collect metric data in this Lab. Click Done. Now, you will see the Instana connection created with the connection status as Not running . After around 30 seconds, you will see that the Instana connection change the connection status to Running as shown below. Now, lets verify that the CP4WAIOps is actually receiving topology data. From the Home page, clik on Resource management under Overview on the left side of the page. On the Resource management page, you will see a new application defined called qotd as shown below: Click on the application qotd and you will see the topology resources related to this application as shown in the picture below. Feel free to zoom-in to see details. Note that it will take 10-20 minutes to get a complete representation of all the topology entities and relationships. Review and Update CP4WAIOps Policies Now, lets take a look at the automations page in CP4WAIOps. By creating automations, we can proactively set up actions and policies to detect and remediate events. From the Home page, clik on Automations under Overview on the left side of the page and make sure you follow the page introduction tour (pop-up) as this will explain this page in detail. In this page, we can manage Policies , Runbooks and Actions . Under the Policies tab, we can see a list of system predefined policies that support a number of features in CP4WAIOps. We will review and enable a predefined policy that create stories: Click on the Tag pull down filter and select Story to only show the Story related policies. Change the state of the DEMO Story creation policy for all alerts policy from Enabled to Disabled by clicking on the State slider. We don't need this policy in this Lab. Change the state of the Default story creation policy for all alerts policy from Disabled to Enabled by clicking on the State slider. We need to enable this policy so we can see stories being created later in the Inference section of the Lab. This Policy will basically create a new Story for every new Alert regardless of the severity of the Alert. Click on the policy name to see the policy details on the right side of the page. Click on the Journal tab to see the updates done so far to this policy. Click on the Specification tab to see the actual steps or logic of the policy. That's all we need to do to enable this policy.","title":"Configuring Data Sources"},{"location":"integration/#configuring-data-sources","text":"","title":"Configuring Data Sources"},{"location":"integration/#connecting-to-the-cp4waiops-console","text":"To run this Lab, make sure that you use the Chrome or Firefox browser and that the language in your browser settings is set to English . Lets start by connecting to the CP4WAIOps console. Use the URL and login credentials given by your Lab coordinator. You will see the console Home page as shown below. Note that the Lab environments use self signed certificates. During login the browser will ask you to accept the risk. You will probably get two prompts to accept the risk. On the login page, select IBM provided credentials (admin only) . As you go to different new pages in the environment, you will see the Page Tour pop-up. Feel free to follow them.","title":"Connecting to the CP4WAIOps console"},{"location":"integration/#configure-the-efk-integration","text":"EFK is a variant of ELK (Elasticsearch, Logstash, and Kibana). EFK is a suite of tools combining Elasticsearch, Fluentd, and Kibana that functions as an application log aggregation tool. Note that Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. Now, to simplify the installation and configuration effort for installing the different components of the EFK stack on OpenShift, we leverage the OpenShift Logging library from OpenShift. OpenShift customers that prefer not to spend part of their budget on a commercial log aggregator such as Humio, Splunk, or LogDNA, more than likely use the OpenShift Logging library that comes out of the box. To have the CP4WAIOps collect logs from the EFK installation that leverages the OpenShift Logging library, you need to define an EFK integration. The lab will provide the values that you should use for defining the EFK integration. Note that we will configure this connection but we will leave the data flow disabled because historical log training data has already been loaded to speed up the Lab. We will enable the data flow later in the Lab during the final Log Anomaly Inference section From the Home page, under Overview clik on Data and tool connections on the left side of the page. Click on the Add connection button on the top right. On the ELK card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Complete the ELK, Add connection form, with the following values: Name : Name of the ELK integration, for example EFK for QOTD . Description : Leave it blank. ELK service URL : Get the service URL for the EFK installation from the Lab Parameters Table . Make sure to include the last * character at the end. Kibana URL : Get URL for Kibana from the Lab Parameters Table . Authentication type : Set this value to Token . Token : Get the token from the Lab Parameters Table . Certificate : Leave it blank. Filters : Don't change it. Time zone : Select GMT-4 . Kibana port : Type 443 . Base parallelism : Don't change it. Sampling rate : Don't change it. JSON processing option : Don't change it. Click on the Test connection button and confirm you get Test succeeded Click on the Next button. Field mapping : Use the mapping shown below instead of the default mapping provided on the ELK integration. Make sure you see the Valid JSON configuration message after that: { \"codec\": \"elk\", \"message_field\": \"message\", \"log_entity_types\": \"kubernetes.container_image_id, kubernetes.host, kubernetes.pod_name, kubernetes.namespace_name\", \"instance_id_field\": \"kubernetes.container_name\", \"rolling_time\": 10, \"timestamp_field\": \"@timestamp\" } Click on the Next button. Data flow : we will leave this Off (grey) as the historical log data has already been loaded to speed up the Lab. We will enable the data flow later in the Lab during the final Log Anomaly Inference section Mode : Select the Historical data for initial AI training option using the dates listed below. CP4WAIOps will ingest one day of historical application log data (stored in the log aggregator) that we know in advance that can be used as a \"reference\" for a normal day because no major IT Operations incident happen during that day. We will use this data later for Log Anomaly training. Start date: May 8, 2022 End Date: May 8, 2022 Source parallelism (1-50) : Don't change it. Click on Done . The following screenshots show the form update flow as guidance (note that config values may be different in the screenshots, follow the instructions instead) After some time, you would see the message Connection completed. IBM Cloud Pak for Watson AIOps has successfully processed your request as shown below.","title":"Configure the EFK Integration"},{"location":"integration/#configure-the-instana-integration","text":"The CP4WAIOps will also consume topology information from Instana therefore we will configure this integration. Lets verify first that there is no topology data in the system. From the navigator menu, go to the Home page again, clik on Resource management under Overview on the left side of the page. On the Resource management page, make sure there are no Applications, Resource groups nor Resources defined, as shown below Now, lets define the Instana integration. From the Home page, clik on Data and tool connections under Overview on the left side of the page. Click on the Add connection button on the top right. On the Instana card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Enter the following Add connection information: Name : The display name of your connection, for example Instana connection . Description : An optional description for the connection. Leave it blank. Endpoint : Get the URL for the Instana Endpoint from the Lab Parameters Table . API Token : Get the API Token from the Lab Parameters Table . Deployment options : Select local (Note that it is possible to deploy an Instana conection gateway remotely, but in this lab we will deploy in the same local cluster). Click Next. Enter the following Collect topology data : Enable data flow : Set this toggle button to on (green). Time window (seconds): Leave as it is. This is the windowSize parameter within the Instana API. Connection Intervals (seconds): Leave as it is. This is how frequently to run the job to collect topology. Application name allowlist pattern : This allows to select from the set of applications that Instana is \"observing\" which one we will pull data from. In this Lab, we will pull topology data from a single application called qotd. Type in qotd . Import Instana application perspectives as Cloud Pak for Watson AIOps applications : Make sure this toggle button is set to on (green). This option will save us some time as we don't need to manually create in CP4WAIOps an application that group the topology resources that we pull from Instana. In CP4WAIOps, an application represents a group of resources put together. Click Next. Enter the following in Collect event data : Enable data flow : Slide this toggle button to off (grey). We will not collect event data in this Lab. Click Next. Enter the following in Collect metric data : Enable data flow : Slide this toggle button to off (grey). We will not collect metric data in this Lab. Click Done. Now, you will see the Instana connection created with the connection status as Not running . After around 30 seconds, you will see that the Instana connection change the connection status to Running as shown below. Now, lets verify that the CP4WAIOps is actually receiving topology data. From the Home page, clik on Resource management under Overview on the left side of the page. On the Resource management page, you will see a new application defined called qotd as shown below: Click on the application qotd and you will see the topology resources related to this application as shown in the picture below. Feel free to zoom-in to see details. Note that it will take 10-20 minutes to get a complete representation of all the topology entities and relationships.","title":"Configure the Instana Integration"},{"location":"integration/#review-and-update-cp4waiops-policies","text":"Now, lets take a look at the automations page in CP4WAIOps. By creating automations, we can proactively set up actions and policies to detect and remediate events. From the Home page, clik on Automations under Overview on the left side of the page and make sure you follow the page introduction tour (pop-up) as this will explain this page in detail. In this page, we can manage Policies , Runbooks and Actions . Under the Policies tab, we can see a list of system predefined policies that support a number of features in CP4WAIOps. We will review and enable a predefined policy that create stories: Click on the Tag pull down filter and select Story to only show the Story related policies. Change the state of the DEMO Story creation policy for all alerts policy from Enabled to Disabled by clicking on the State slider. We don't need this policy in this Lab. Change the state of the Default story creation policy for all alerts policy from Disabled to Enabled by clicking on the State slider. We need to enable this policy so we can see stories being created later in the Inference section of the Lab. This Policy will basically create a new Story for every new Alert regardless of the severity of the Alert. Click on the policy name to see the policy details on the right side of the page. Click on the Journal tab to see the updates done so far to this policy. Click on the Specification tab to see the actual steps or logic of the policy. That's all we need to do to enable this policy.","title":"Review and Update CP4WAIOps Policies"},{"location":"log-anomaly/","text":"Log Anomaly Training Recap Lets recap what we have done so far: We have defined an integration to a Log Aggregator (EFK) that its receiving logs from an application called qotd and we have pulled logs into CP4WAIOps for a period of 1 day (in reality you did not save this integration at the end since the log data is already loaded as part of the lab) We have defined an integration to Instana which is observing the environment where the qotd application is running and we have pulled every resource topology information related to this application into the CP4WAIOps. We have enabled one CP4WAIOps policy to create Stories as soon as an Alert is received. Now we will do log anomaly training to create a model that CP4WAIOps will use to find anomalies in the logs. Log Anomaly Algorithms In the CP4WAIOps, two log anomaly detection AI algorithms are available, each of which can run independently of the other. If both algorithms are enabled, then any log anomalies that are discovered by both algorithms are reconciled so that only one alert is generated. The severity of this combined alert is equal to the highest severity of the two alerts. The two algorithms are: Statistical baseline log anomaly detection extracts specific entities from the logs, such as short text strings that indicate error codes and exceptions. The algorithm combines the entities with other statistics, such as number of logs for each component, and uses the data as a baseline to discover abnormal behavior in your live log data. Natural language log anomaly detection uses natural language techniques on a subset of your log data to discover abnormal behavior. Log anomaly detection takes large amounts of log data to learn what is considered a normal behavior for a particular component. This model goes beyond just looking at error states or frequency of metadata around log messages. Instead, it determines when something becomes an anomaly compared to what patterns it typically exhibits during normal times. For example, if a minor error message shows up 2-3 times during a normal day this pattern can be learned by the model so in the future if the same message shows up 15 times during the day, then it will be flag as an anomaly and an Alert will be created. Configuring Log Anomaly Training From the Home page, under Overview click on AI model management to open the AI Model Management dashboard and make sure you follow the Tour pop-up window. On the Training tab, on the Log anomaly detection - natural language tile, click Set up training . Getting started: You will see the connection here that we just defined. Remember we have disabled the data flow because the historical log training data is loaded already. Click Next to move to the next pane. Select data: Specify a date range to train upon. Select Custom and specify the dates listed below. Note that from the full day of application logs that we pulled from the log aggregator (preloaded data), we will select the full 24 hours of data. In a real production deployment, you will tipically pull and later select up to a week of data to have a more representative model. Start Date 05/06/2022 12:00 AM (UTC +00:00) End Date 05/09/2022 12:00 AM (UTC +00:00) Click Next to move to the next pane. Filter data: Here we have the option to filter out known anomalies in the training data by specifing date ranges to skip. In this Lab, we don't need to specify any date range to get filtered. Click Next to move to the next pane. Schedule training: Here we can specify a time schedule to run the training if needed. In this Lab we will run the training manually therefore make sure that Schedule training is set to Off (grey). Click Next to move to the next pane. Deploy: To review the results of training before deploying the model, ensure that Deployment type is set to Review first . Click Done to save the algorithm configuration. Now you will notice that the Log anomaly detection - natural language tile, is set to Not started . Now that the Log Anomaly configuration is finished, we can trigger the training. Train & Deploy a Log Anomaly Model From the AI model management page, click on the three dots in the Log anomaly detection - natural language tile and select View details . This page shows the log anomaly training configuration that we just finished. In the right sidebar, click Start training . After a few moments, the following response is displayed: Training successfully started . The AI Training tile displays various training status messages: such as Queued . You will see in the Models tile, how the wheel chart turns from red to green as diferent resources are added to the model. Click on View resources to see the details on the resource training. Click on Show details in the Resources pop-up to learn more about this page. Its OK if the anomaly resource name fails to get a model. There are not enough log lines that have information about this resource. Log Anomaly Training will typically take around 30 minutes for this amount of data. This a good time to step away and come back in 30 minutes. Once the training is complete, the Training status tile displays the message: 3 of 3 complete . At this point, the log anomaly models are created. Now the last step in the Log Anomaly Training section is to deploy the model. Click on the Versions tab and confirm there is a model created with Status Complete . Click on the 3 dots on the right and select Deploy as shown below. Once the model is deployed, you will see the model Status changed to Deployed as shown below.","title":"Log Anomaly Training"},{"location":"log-anomaly/#log-anomaly-training","text":"","title":"Log Anomaly Training"},{"location":"log-anomaly/#recap","text":"Lets recap what we have done so far: We have defined an integration to a Log Aggregator (EFK) that its receiving logs from an application called qotd and we have pulled logs into CP4WAIOps for a period of 1 day (in reality you did not save this integration at the end since the log data is already loaded as part of the lab) We have defined an integration to Instana which is observing the environment where the qotd application is running and we have pulled every resource topology information related to this application into the CP4WAIOps. We have enabled one CP4WAIOps policy to create Stories as soon as an Alert is received. Now we will do log anomaly training to create a model that CP4WAIOps will use to find anomalies in the logs.","title":"Recap"},{"location":"log-anomaly/#log-anomaly-algorithms","text":"In the CP4WAIOps, two log anomaly detection AI algorithms are available, each of which can run independently of the other. If both algorithms are enabled, then any log anomalies that are discovered by both algorithms are reconciled so that only one alert is generated. The severity of this combined alert is equal to the highest severity of the two alerts. The two algorithms are: Statistical baseline log anomaly detection extracts specific entities from the logs, such as short text strings that indicate error codes and exceptions. The algorithm combines the entities with other statistics, such as number of logs for each component, and uses the data as a baseline to discover abnormal behavior in your live log data. Natural language log anomaly detection uses natural language techniques on a subset of your log data to discover abnormal behavior. Log anomaly detection takes large amounts of log data to learn what is considered a normal behavior for a particular component. This model goes beyond just looking at error states or frequency of metadata around log messages. Instead, it determines when something becomes an anomaly compared to what patterns it typically exhibits during normal times. For example, if a minor error message shows up 2-3 times during a normal day this pattern can be learned by the model so in the future if the same message shows up 15 times during the day, then it will be flag as an anomaly and an Alert will be created.","title":"Log Anomaly Algorithms"},{"location":"log-anomaly/#configuring-log-anomaly-training","text":"From the Home page, under Overview click on AI model management to open the AI Model Management dashboard and make sure you follow the Tour pop-up window. On the Training tab, on the Log anomaly detection - natural language tile, click Set up training . Getting started: You will see the connection here that we just defined. Remember we have disabled the data flow because the historical log training data is loaded already. Click Next to move to the next pane. Select data: Specify a date range to train upon. Select Custom and specify the dates listed below. Note that from the full day of application logs that we pulled from the log aggregator (preloaded data), we will select the full 24 hours of data. In a real production deployment, you will tipically pull and later select up to a week of data to have a more representative model. Start Date 05/06/2022 12:00 AM (UTC +00:00) End Date 05/09/2022 12:00 AM (UTC +00:00) Click Next to move to the next pane. Filter data: Here we have the option to filter out known anomalies in the training data by specifing date ranges to skip. In this Lab, we don't need to specify any date range to get filtered. Click Next to move to the next pane. Schedule training: Here we can specify a time schedule to run the training if needed. In this Lab we will run the training manually therefore make sure that Schedule training is set to Off (grey). Click Next to move to the next pane. Deploy: To review the results of training before deploying the model, ensure that Deployment type is set to Review first . Click Done to save the algorithm configuration. Now you will notice that the Log anomaly detection - natural language tile, is set to Not started . Now that the Log Anomaly configuration is finished, we can trigger the training.","title":"Configuring Log Anomaly Training"},{"location":"log-anomaly/#train-deploy-a-log-anomaly-model","text":"From the AI model management page, click on the three dots in the Log anomaly detection - natural language tile and select View details . This page shows the log anomaly training configuration that we just finished. In the right sidebar, click Start training . After a few moments, the following response is displayed: Training successfully started . The AI Training tile displays various training status messages: such as Queued . You will see in the Models tile, how the wheel chart turns from red to green as diferent resources are added to the model. Click on View resources to see the details on the resource training. Click on Show details in the Resources pop-up to learn more about this page. Its OK if the anomaly resource name fails to get a model. There are not enough log lines that have information about this resource. Log Anomaly Training will typically take around 30 minutes for this amount of data. This a good time to step away and come back in 30 minutes. Once the training is complete, the Training status tile displays the message: 3 of 3 complete . At this point, the log anomaly models are created. Now the last step in the Log Anomaly Training section is to deploy the model. Click on the Versions tab and confirm there is a model created with Status Complete . Click on the 3 dots on the right and select Deploy as shown below. Once the model is deployed, you will see the model Status changed to Deployed as shown below.","title":"Train &amp; Deploy a Log Anomaly Model"}]}