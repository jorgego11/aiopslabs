{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Cloud Pack for Watson AIOps v3.3 Lab For the complete product documentation, visit this link . Lab Overview In this Lab, you will play the role of an IT Operations expert that will configure the Cloud Pack for Watson AIOps (CP4WAIOps) to support IT Operations of a sample business application called QOTD. This application is being \"observed\" by Instana to pull application performance management metrics including topology. The application logs are being collected by a ElasticSearch (ELK) based log aggregator. These logs are consumed by CP4WAIOps to create a Log Anomaly model. And this model is used to detect application anomalies that will be shown in an incident Story. These are the different Lab steps: Configure the EFK Integration in CP4WAIOps to pull business application logs Configure the Instana Integration in CP4WAIOps to pull business application topology Update CP4WAIOps Policies to create stories Configure CP4WAIOps Log Anomaly Training Configure CP4WAIOps Log Anomaly Inference Verify CP4WAIOps incident stories NOTE: The Lab has configuration parameters that you should copy/paste from the tables shown in this web page The Lab call these as Lab Parameters Table . Make sure you copy the full parameter value and not miss any characters. The following deployment architecture shows the different components that support the Lab: Lab Terms Definition Lets define some key CP4WAIOps concepts and terms that will help you understand the Lab: Events: An event is a record containing structured data summarizing key attributes of an occurrence on a managed entity, which might be a network resource, some part of that resource, or other key element associated with your network, services, or applications. An event may or may not indicate something anomalous and is a point-in-time, immutable statement about the managed entity in question. Alerts: Alerts are created when one or more events indicate an anomalous condition. Alerts represent an ongoing anomalous condition against a single managed entity. Unlike events, alerts might evolve over time as the condition changes. Alerts have a start and an end time. The creation and evolution of alerts are informed by events. CP4WAIOps automatically correlates alerts to determine what alerts are likely to share a common cause. This is determined based on a combination of: Scope-based correlation - Any alerts which have the same value for the resource field are correlated. Temporal correlation - The system continually analyzes past alerts to determine which alerts tend to frequently co-occur. When these alerts occur together again, they are correlated. Topological correlation - Any alerts which refer to resources within the same Resource group are correlated. Stories: Stories represent the context around an incident which is currently severely impacting IT Operations. This includes all alerts that are related to the incident and information about how the affected resources are related. The creation and evolution of stories are informed by alerts . Stories can help build the understanding of the current situation and also drive the remediation steps. In other words, stories represent an IT Operations incident and are categorized by priority from 1 (high) to 5 (low). Policies: Policies are rules that contain a condition and a set of actions that can be manual or automated. They can be triggered to automatically promote events to alerts, reduce noise by grouping alerts into a story, and assign runbooks to remediate alerts. Each policy has an execution priority number which is determines which policy runs first. Runbooks: A runbook is a controlled set of automated and manual steps that support system and network operational processes. A runbook orchestrates all types of infrastructure elements, such as applications, network components, or servers. We can also use runbooks to document standard procedures that can be leveraged by IT operations. Actions: Actions are the collection of several predefined steps into a single automated tested entity that can be shared by multiple runbooks. Actions improve runbook efficiency by encapsulating procedures and operations.","title":"Welcome to the Cloud Pack for Watson AIOps v3.3 Lab"},{"location":"#welcome-to-the-cloud-pack-for-watson-aiops-v33-lab","text":"For the complete product documentation, visit this link .","title":"Welcome to the Cloud Pack for Watson AIOps v3.3 Lab"},{"location":"#lab-overview","text":"In this Lab, you will play the role of an IT Operations expert that will configure the Cloud Pack for Watson AIOps (CP4WAIOps) to support IT Operations of a sample business application called QOTD. This application is being \"observed\" by Instana to pull application performance management metrics including topology. The application logs are being collected by a ElasticSearch (ELK) based log aggregator. These logs are consumed by CP4WAIOps to create a Log Anomaly model. And this model is used to detect application anomalies that will be shown in an incident Story. These are the different Lab steps: Configure the EFK Integration in CP4WAIOps to pull business application logs Configure the Instana Integration in CP4WAIOps to pull business application topology Update CP4WAIOps Policies to create stories Configure CP4WAIOps Log Anomaly Training Configure CP4WAIOps Log Anomaly Inference Verify CP4WAIOps incident stories NOTE: The Lab has configuration parameters that you should copy/paste from the tables shown in this web page The Lab call these as Lab Parameters Table . Make sure you copy the full parameter value and not miss any characters. The following deployment architecture shows the different components that support the Lab:","title":"Lab Overview"},{"location":"#lab-terms-definition","text":"Lets define some key CP4WAIOps concepts and terms that will help you understand the Lab: Events: An event is a record containing structured data summarizing key attributes of an occurrence on a managed entity, which might be a network resource, some part of that resource, or other key element associated with your network, services, or applications. An event may or may not indicate something anomalous and is a point-in-time, immutable statement about the managed entity in question. Alerts: Alerts are created when one or more events indicate an anomalous condition. Alerts represent an ongoing anomalous condition against a single managed entity. Unlike events, alerts might evolve over time as the condition changes. Alerts have a start and an end time. The creation and evolution of alerts are informed by events. CP4WAIOps automatically correlates alerts to determine what alerts are likely to share a common cause. This is determined based on a combination of: Scope-based correlation - Any alerts which have the same value for the resource field are correlated. Temporal correlation - The system continually analyzes past alerts to determine which alerts tend to frequently co-occur. When these alerts occur together again, they are correlated. Topological correlation - Any alerts which refer to resources within the same Resource group are correlated. Stories: Stories represent the context around an incident which is currently severely impacting IT Operations. This includes all alerts that are related to the incident and information about how the affected resources are related. The creation and evolution of stories are informed by alerts . Stories can help build the understanding of the current situation and also drive the remediation steps. In other words, stories represent an IT Operations incident and are categorized by priority from 1 (high) to 5 (low). Policies: Policies are rules that contain a condition and a set of actions that can be manual or automated. They can be triggered to automatically promote events to alerts, reduce noise by grouping alerts into a story, and assign runbooks to remediate alerts. Each policy has an execution priority number which is determines which policy runs first. Runbooks: A runbook is a controlled set of automated and manual steps that support system and network operational processes. A runbook orchestrates all types of infrastructure elements, such as applications, network components, or servers. We can also use runbooks to document standard procedures that can be leveraged by IT operations. Actions: Actions are the collection of several predefined steps into a single automated tested entity that can be shared by multiple runbooks. Actions improve runbook efficiency by encapsulating procedures and operations.","title":"Lab Terms Definition"},{"location":"about/","text":"Lab: Cloud Pak for Watson AIOps v3.3 - Log Anomaly + Instana topology Author: Jorge Gonzalez Orozco Date: May 2022 \u00a9 Copyright IBM Corp. 2022","title":"About"},{"location":"inference/","text":"Log Anomaly Inference Recap Lets recap what we have done so far: We have defined an integration to a Log Aggregator (EFK) that received historical application logs for a \"normal\" day of operations of a sample application called qotd . We have defined an integration to Instana which is observing the environment where the sample qotd application is running and we have pulled every resource topology information related to this application into CP4WAIOps. We have enabled one CP4WAIOps policy to create Stories. We have created a log anomaly model by training on one day of \"normal\" log data from the qotd application. We have deployed this model so we can actually test it with real time log data. Start Log Anomaly Inference on Real-Time Logs Now that our log anomaly trained model is successfully deployed, its ready to start doing log anomaly inference. That is, \"scan\" logs coming in real time from the sample application qotd and create alerts when the real time logs deviate from what is considered \"normal\" in the trained model. The final important point here is that, during the Lab execution, we are injecting real time anomalies into the sample application qotd, for example we are shutting down API services, increasing service responce latency, etc. This concept is similar to what the IT Ops team at Netflix call Chaos Monkey . These anomalies will be detected by the CP4WAIOps and you will see the new alerts and stories created. We are going to modify our integration with the log aggregator to start consuming real-time logs instead historical logs as we did previously. From the Home page, under Overview clik on Data and tool connections on the left side of the page. Click on the ELK connection type. Click on the EFK for QOTD connection definition. Click the Next button twice to arrive to the AI training and log data page. In the AI training and log data page: Make sure the Data flow is set to On . Choose Live data for continuous AI training and anomaly detection as the option for training mode. The AI training and log data page should look like the screenshot below: Finally click on the Save button. After saving the configuration, make sure the ELK integration page shows the Data Flow Status as Running as shown below: Verify Incident Story After deploying the log anomaly model and enabling real-time log consumption, we need to wait a few minutes to see the new alerts and stories being created. From the Home page, under Overview clik on Stories and alerts on the left side of the page. On the Stories tab, make sure you see one or more stories created, as shown below. Confirm that the story has been recently created. If you don't see any stories, just wait a few minutes. As we mentioned before during the definition of the terms, from an IT Operations point of view we have Events that trigger Alerts that trigger the creation of stories Stories . Alerts tab: Click on the Alerts tab. Here you will see one or more alerts, as shown below: These are the Log Anomaly alerts that were created by the Log Anomaly model we just deployed. They were created because the real-time logs shown patterns that are not considered \"normal\" by the trained model. Click on the first alert under the Summary column to explore the alert details on the right side of the page. On the Alert details under Properties review the values of the different properties. Look at the value of the property eventCount , this is the number of events that were aggregated into a single Alert. Stories tab: Click back on the Stories tab, these are the stories that were created based on the alerts that you just saw before. In a real production environment, a story can group and correlate different but related alerts into a single story page. For example, lets imagine that the storage of a key application database goes down. In this scenario, we could have alerts from the database monitoring platformm, log anomaly alerts from the APIs that use the database and web page response alerts that shows database data. All these three alerts will be combined into a single story in order to help the IT Operations personnel find the root cause the the incident. Click on the story Description to look at the story details. You will see the story page as shown below: On the left side, we see the Top alerts that triggered this story. These alerts are ranked by probable cause showing the top one as the most probable root cause of the overall incident. In the middle of the page, we see topology information that was provided by Instana with the problematic resource highlighted. The top right side of the page shows the StoryID, the story owner (Unassigned) and the priority. The Change story settings button on the top right allow us to change the priority, the status and assignee. The Alerts tab shows only the alerts related to these particular story. The Topology tab shows the overall topology context where this incident happen (NOTE that currently this tab is not showing any data. The overall topology context can be seen from Home -> Resource management -> qotd) CONGRATULATIONS, THIS IS THE END OF THE LAB","title":"Log Anomaly Inference"},{"location":"inference/#log-anomaly-inference","text":"","title":"Log Anomaly Inference"},{"location":"inference/#recap","text":"Lets recap what we have done so far: We have defined an integration to a Log Aggregator (EFK) that received historical application logs for a \"normal\" day of operations of a sample application called qotd . We have defined an integration to Instana which is observing the environment where the sample qotd application is running and we have pulled every resource topology information related to this application into CP4WAIOps. We have enabled one CP4WAIOps policy to create Stories. We have created a log anomaly model by training on one day of \"normal\" log data from the qotd application. We have deployed this model so we can actually test it with real time log data.","title":"Recap"},{"location":"inference/#start-log-anomaly-inference-on-real-time-logs","text":"Now that our log anomaly trained model is successfully deployed, its ready to start doing log anomaly inference. That is, \"scan\" logs coming in real time from the sample application qotd and create alerts when the real time logs deviate from what is considered \"normal\" in the trained model. The final important point here is that, during the Lab execution, we are injecting real time anomalies into the sample application qotd, for example we are shutting down API services, increasing service responce latency, etc. This concept is similar to what the IT Ops team at Netflix call Chaos Monkey . These anomalies will be detected by the CP4WAIOps and you will see the new alerts and stories created. We are going to modify our integration with the log aggregator to start consuming real-time logs instead historical logs as we did previously. From the Home page, under Overview clik on Data and tool connections on the left side of the page. Click on the ELK connection type. Click on the EFK for QOTD connection definition. Click the Next button twice to arrive to the AI training and log data page. In the AI training and log data page: Make sure the Data flow is set to On . Choose Live data for continuous AI training and anomaly detection as the option for training mode. The AI training and log data page should look like the screenshot below: Finally click on the Save button. After saving the configuration, make sure the ELK integration page shows the Data Flow Status as Running as shown below:","title":"Start Log Anomaly Inference on Real-Time Logs"},{"location":"inference/#verify-incident-story","text":"After deploying the log anomaly model and enabling real-time log consumption, we need to wait a few minutes to see the new alerts and stories being created. From the Home page, under Overview clik on Stories and alerts on the left side of the page. On the Stories tab, make sure you see one or more stories created, as shown below. Confirm that the story has been recently created. If you don't see any stories, just wait a few minutes. As we mentioned before during the definition of the terms, from an IT Operations point of view we have Events that trigger Alerts that trigger the creation of stories Stories . Alerts tab: Click on the Alerts tab. Here you will see one or more alerts, as shown below: These are the Log Anomaly alerts that were created by the Log Anomaly model we just deployed. They were created because the real-time logs shown patterns that are not considered \"normal\" by the trained model. Click on the first alert under the Summary column to explore the alert details on the right side of the page. On the Alert details under Properties review the values of the different properties. Look at the value of the property eventCount , this is the number of events that were aggregated into a single Alert. Stories tab: Click back on the Stories tab, these are the stories that were created based on the alerts that you just saw before. In a real production environment, a story can group and correlate different but related alerts into a single story page. For example, lets imagine that the storage of a key application database goes down. In this scenario, we could have alerts from the database monitoring platformm, log anomaly alerts from the APIs that use the database and web page response alerts that shows database data. All these three alerts will be combined into a single story in order to help the IT Operations personnel find the root cause the the incident. Click on the story Description to look at the story details. You will see the story page as shown below: On the left side, we see the Top alerts that triggered this story. These alerts are ranked by probable cause showing the top one as the most probable root cause of the overall incident. In the middle of the page, we see topology information that was provided by Instana with the problematic resource highlighted. The top right side of the page shows the StoryID, the story owner (Unassigned) and the priority. The Change story settings button on the top right allow us to change the priority, the status and assignee. The Alerts tab shows only the alerts related to these particular story. The Topology tab shows the overall topology context where this incident happen (NOTE that currently this tab is not showing any data. The overall topology context can be seen from Home -> Resource management -> qotd) CONGRATULATIONS, THIS IS THE END OF THE LAB","title":"Verify Incident Story"},{"location":"integration/","text":"Configuring Data Sources Connecting to the CP4WAIOps console Lets start by connecting to the CP4WAIOps console. Use the URL and login credentials given by your Lab coordinator. You will see the console home page as shown below. Configure the EFK Integration EFK is a variant of ELK (Elasticsearch, Logstash, and Kibana). EFK is a suite of tools combining Elasticsearch, Fluentd, and Kibana that functions as a log aggregation tool. Note that Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. Now, to simplify the installation and configuration effort for installing the different components of the EFK stack on OpenShift, we leverage the OpenShift Logging library from OpenShift. OpenShift customers that prefer not to spend part of their budget on a commercial log aggregator such as Humio, Splunk, or LogDNA, more than likely use the OpenShift Logging library. To have AI Manager collect logs from the EFK installation that leverages the OpenShift Logging library, you need to define an EFK integration. The lab will provide the values that you should use for defining the EFK integration. From the Home page, under Overview clik on Data and tool connections on the left side of the page. Click on the Add connection button on the top right. On the ELK card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Complete the ELK, Add connection form, with the following values: Name : Name of the ELK integration, for example EFK for QOTD . Description : Leave it blank. ELK service URL : Get the service URL for the EFK installation from the Lab Parameters Table . Kibana URL : Get URL for Kibana from the Lab Parameters Table . Authentication type : Set this value to Token . Token : Get the token from the Lab Parameters Table . Certificate : Leave it blank. Filters : Don't change it. Time zone : Select GMT-4 . Kibana port : Type 443 . Base parallelism : Don't change it. Sampling rate : Don't change it. Click on the Next button. Field mapping : Use the mapping shown below instead of the default mapping provided on the ELK integration. Make sure you see the Valid JSON configuration message after that: { \"codec\": \"elk\", \"message_field\": \"message\", \"log_entity_types\": \"kubernetes.container_image_id, kubernetes.host, kubernetes.pod_name, kubernetes.namespace_name\", \"instance_id_field\": \"kubernetes.container_name\", \"rolling_time\": 10, \"timestamp_field\": \"@timestamp\" } Click on the Test Connection button and confirm you get Test Succeded Click on the Next button. Data flow : Turn this on. Mode : Select the Historical data for initial AI training option using the dates listed below. CP4WAIOps will ingest one day of historical application log data (stored in the log aggregator) that we know in advanced that can be used as a \"reference\" for a normal day because no major IT incident happen. We will use this data later for Log Anomaly training. Start date: May 8, 2022 End Date: May 8, 2022 Source parallelism (1-50) : Don't change it. The following screenshots show the form update flow as guidance (note that config values may be different in the screenshots, follow the previous instructions instead) Finally, click on the Done button. After some time, you will see the message Connection completed. IBM Cloud Pak for Watson AIOps has successfully processed your request . Click on the ELK Connection type and then the ELK integration page will show the EFK for QOTD integration you just defined showing the Data Flow Status as Running as shown below: After around 15 minutes, it will stop pulling the data and the Data Flow Status will change to Finished as shown below. Don't wait. Move to the next step in the meantime. Configure the Instana Integration The CP4WAIOps will consume topology information from Instana therefore we will configure this integration. Lets verify first that there is no topology data in the system. From the navigator menu, go to the Home page again, clik on Resource management under Overview on the left side of the page. On the Resource management page, make sure there are no Applications, Resource groups nor Resources defined, as shown below Now, lets define the Instana integration. From the Home page, clik on Data and tool connections under Overview on the left side of the page. Click on the Add connection button on the top right. On the Instana card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Enter the following Add connection information: Name : The display name of your connection, for example Instana connection . Description : An optional description for the connection. Leave it blank. Endpoint : Get the URL for the Instana Endpoint from the Lab Parameters Table . API Token : Get the API Token from the Lab Parameters Table . Deployment options : Select local (Note that it is possible to deploy an Instana conection gateway remotely, but in this lab we will deploy in the same local cluster). Click Next. Enter the following Collect topology data : Enable data flow : Set this toggle button to on (green). Time window (seconds): Leave as it is. This is the windowSize parameter within the Instana API. Connection Intervals (seconds): Leave as it is. This is how frequently to run the job to collect topology. Application name allowlist pattern : This allows to select from the set of applications that Instana is \"observing\" which one we will pull data from. In this Lab, we will pull topology data from a single application called qotd. Type in qotd . Import Instana application perspectives as Cloud Pak for Watson AIOps applications : Make sure this toggle button is set to on (green). This option will save us some time as we don't need to manually create in CP4WAIOps an application that group the topology resources that we pull from Instana. In CP4WAIOps, an application represents a group of resources put together. Click Next. Enter the following in Collect event data : Enable data flow : Slide this toggle button to off (grey). We will not collect event data in this Lab. Click Next. Enter the following in Collect metric data : Enable data flow : Slide this toggle button to off (grey). We will not collect metric data in this Lab. Click Done. Now, lets verify that CP4WAIOps actually received topology data. From the Home page, clik on Resource management under Overview on the left side of the page. On the Resource management page, you will see a new application defined called qotd as shown below: Click on the application qotd and you will see the topology resources related to this application as shown in the picture below. Feel free to zoom-in to see details. Note that it will take 10-20 minutes to get a complete representation of all the topology entities and relationships. Review and Update CP4WAIOps Policies Now, lets take a look at the automations page in CP4WAIOps. By creating automations, we can proactively set up actions and policies to detect and remediate events. From the Home page, clik on Automations under Overview on the left side of the page. In this page, we can manage Policies , Runbooks and Actions . Under the Policies tab, we can see a list of system predefined policies that support a number of features in CP4WAIOps. We will review and enable a predefined policy that create stories: Click on the Tag pull down filter and select Story to only show the Story related policies. Change the state of the Default story creation policy for all alerts policy from Disabled to Enabled by clicking on the State slider. We need to enable this policy so we can see stories being created later in the Inference section of the Lab. This Policy will basically create a new Story for every new Alert regardless of the severity of the Alert. Click on the policy name to see the policy details on the right side of the page. Click on the Journal tab to see the updates done so far to this policy. Click on the Specification tab to see the actual steps or logic of the policy. That's all we need to do to enable this policy.","title":"Configuring Data Sources"},{"location":"integration/#configuring-data-sources","text":"","title":"Configuring Data Sources"},{"location":"integration/#connecting-to-the-cp4waiops-console","text":"Lets start by connecting to the CP4WAIOps console. Use the URL and login credentials given by your Lab coordinator. You will see the console home page as shown below.","title":"Connecting to the CP4WAIOps console"},{"location":"integration/#configure-the-efk-integration","text":"EFK is a variant of ELK (Elasticsearch, Logstash, and Kibana). EFK is a suite of tools combining Elasticsearch, Fluentd, and Kibana that functions as a log aggregation tool. Note that Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. Now, to simplify the installation and configuration effort for installing the different components of the EFK stack on OpenShift, we leverage the OpenShift Logging library from OpenShift. OpenShift customers that prefer not to spend part of their budget on a commercial log aggregator such as Humio, Splunk, or LogDNA, more than likely use the OpenShift Logging library. To have AI Manager collect logs from the EFK installation that leverages the OpenShift Logging library, you need to define an EFK integration. The lab will provide the values that you should use for defining the EFK integration. From the Home page, under Overview clik on Data and tool connections on the left side of the page. Click on the Add connection button on the top right. On the ELK card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Complete the ELK, Add connection form, with the following values: Name : Name of the ELK integration, for example EFK for QOTD . Description : Leave it blank. ELK service URL : Get the service URL for the EFK installation from the Lab Parameters Table . Kibana URL : Get URL for Kibana from the Lab Parameters Table . Authentication type : Set this value to Token . Token : Get the token from the Lab Parameters Table . Certificate : Leave it blank. Filters : Don't change it. Time zone : Select GMT-4 . Kibana port : Type 443 . Base parallelism : Don't change it. Sampling rate : Don't change it. Click on the Next button. Field mapping : Use the mapping shown below instead of the default mapping provided on the ELK integration. Make sure you see the Valid JSON configuration message after that: { \"codec\": \"elk\", \"message_field\": \"message\", \"log_entity_types\": \"kubernetes.container_image_id, kubernetes.host, kubernetes.pod_name, kubernetes.namespace_name\", \"instance_id_field\": \"kubernetes.container_name\", \"rolling_time\": 10, \"timestamp_field\": \"@timestamp\" } Click on the Test Connection button and confirm you get Test Succeded Click on the Next button. Data flow : Turn this on. Mode : Select the Historical data for initial AI training option using the dates listed below. CP4WAIOps will ingest one day of historical application log data (stored in the log aggregator) that we know in advanced that can be used as a \"reference\" for a normal day because no major IT incident happen. We will use this data later for Log Anomaly training. Start date: May 8, 2022 End Date: May 8, 2022 Source parallelism (1-50) : Don't change it. The following screenshots show the form update flow as guidance (note that config values may be different in the screenshots, follow the previous instructions instead) Finally, click on the Done button. After some time, you will see the message Connection completed. IBM Cloud Pak for Watson AIOps has successfully processed your request . Click on the ELK Connection type and then the ELK integration page will show the EFK for QOTD integration you just defined showing the Data Flow Status as Running as shown below: After around 15 minutes, it will stop pulling the data and the Data Flow Status will change to Finished as shown below. Don't wait. Move to the next step in the meantime.","title":"Configure the EFK Integration"},{"location":"integration/#configure-the-instana-integration","text":"The CP4WAIOps will consume topology information from Instana therefore we will configure this integration. Lets verify first that there is no topology data in the system. From the navigator menu, go to the Home page again, clik on Resource management under Overview on the left side of the page. On the Resource management page, make sure there are no Applications, Resource groups nor Resources defined, as shown below Now, lets define the Instana integration. From the Home page, clik on Data and tool connections under Overview on the left side of the page. Click on the Add connection button on the top right. On the Instana card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Enter the following Add connection information: Name : The display name of your connection, for example Instana connection . Description : An optional description for the connection. Leave it blank. Endpoint : Get the URL for the Instana Endpoint from the Lab Parameters Table . API Token : Get the API Token from the Lab Parameters Table . Deployment options : Select local (Note that it is possible to deploy an Instana conection gateway remotely, but in this lab we will deploy in the same local cluster). Click Next. Enter the following Collect topology data : Enable data flow : Set this toggle button to on (green). Time window (seconds): Leave as it is. This is the windowSize parameter within the Instana API. Connection Intervals (seconds): Leave as it is. This is how frequently to run the job to collect topology. Application name allowlist pattern : This allows to select from the set of applications that Instana is \"observing\" which one we will pull data from. In this Lab, we will pull topology data from a single application called qotd. Type in qotd . Import Instana application perspectives as Cloud Pak for Watson AIOps applications : Make sure this toggle button is set to on (green). This option will save us some time as we don't need to manually create in CP4WAIOps an application that group the topology resources that we pull from Instana. In CP4WAIOps, an application represents a group of resources put together. Click Next. Enter the following in Collect event data : Enable data flow : Slide this toggle button to off (grey). We will not collect event data in this Lab. Click Next. Enter the following in Collect metric data : Enable data flow : Slide this toggle button to off (grey). We will not collect metric data in this Lab. Click Done. Now, lets verify that CP4WAIOps actually received topology data. From the Home page, clik on Resource management under Overview on the left side of the page. On the Resource management page, you will see a new application defined called qotd as shown below: Click on the application qotd and you will see the topology resources related to this application as shown in the picture below. Feel free to zoom-in to see details. Note that it will take 10-20 minutes to get a complete representation of all the topology entities and relationships.","title":"Configure the Instana Integration"},{"location":"integration/#review-and-update-cp4waiops-policies","text":"Now, lets take a look at the automations page in CP4WAIOps. By creating automations, we can proactively set up actions and policies to detect and remediate events. From the Home page, clik on Automations under Overview on the left side of the page. In this page, we can manage Policies , Runbooks and Actions . Under the Policies tab, we can see a list of system predefined policies that support a number of features in CP4WAIOps. We will review and enable a predefined policy that create stories: Click on the Tag pull down filter and select Story to only show the Story related policies. Change the state of the Default story creation policy for all alerts policy from Disabled to Enabled by clicking on the State slider. We need to enable this policy so we can see stories being created later in the Inference section of the Lab. This Policy will basically create a new Story for every new Alert regardless of the severity of the Alert. Click on the policy name to see the policy details on the right side of the page. Click on the Journal tab to see the updates done so far to this policy. Click on the Specification tab to see the actual steps or logic of the policy. That's all we need to do to enable this policy.","title":"Review and Update CP4WAIOps Policies"},{"location":"log-anomaly/","text":"Log Anomaly Training Recap Lets recap what we have done so far: We have defined an integration to a Log Aggregator (EFK) that its receiving logs from an application called qotd and we have pulled logs into CP4WAIOps for a period of 1 day. We have defined an integration to Instana which is observing the environment where qotd is running and we have pulled every resource topology information related to this application into CP4WAIOps. We have enabled one CP4WAIOps policy to create Stories. Now we will do log anomaly training to create a model that CP4WAIOps will use to find anomalies in the logs. Log Anomaly Algorithms In CP4WAIOps v3.3, two log anomaly detection AI algorithms are available, each of which can run independently of the other. If both algorithms are enabled, then any log anomalies that are discovered by both algorithms are reconciled so that only one alert is generated. The severity of this combined alert is equal to the highest severity of the two alerts. Statistical baseline log anomaly detection extracts specific entities from the logs, such as short text strings that indicate error codes and exceptions. The algorithm combines the entities with other statistics, such as number of logs for each component, and uses the data as a baseline to discover abnormal behavior in your live log data. Natural language log anomaly detection uses natural language techniques on a subset of your log data to discover abnormal behavior. Log anomaly detection takes large amounts of log data and trains on it to learn what is considered normal behavior for a particular component. This model goes beyond just looking at error states or frequency of metadata around log messages. Instead, it determines when something becomes an anomaly compared to what patterns it typically exhibits during normal times. Configuring Log Anomaly Training From the Home page, under Overview click on AI model management to open the AI Model Management dashboard (feel free to skip the Tour pop-up window). On the AI algorithms tab, on the Log anomaly detection-natural language tile, click Configure . Getting started: Under Data and tool connections, check that at least one connection is listed down the page. Click Next to move to the next pane. Provide details: Here we provide a name and description to identify this algorithm configuration later. For Configuration name, type QOTD-NLP-model . Leave the Configuration description empty. Click Next to move to the next pane. Select data: Specify a date range to train upon. Select Custom and specify the dates listed below. Note that from the full day of application logs that we pulled from the log aggregator, we will select the full 24 hours of data. In a real production deployment, you will tipically pull and later select up to a week of data to have a more representative model. Start Date 05/08/2022 12:00 AM Local Time (UTC -04:00) End Date 05/09/2022 12:00 AM Local Time (UTC -04:00) Click Next to move to the next pane. Filter data: Here we have the option to filter out known anomalies in the training data by specifing date ranges to skip. In this Lab, we don't need to specify any date range to get filtered. Click Next to move to the next pane. Schedule training: Here we can specify a time schedule to run the training if needed. In this Lab we will run the training manually therefore make sure that Run on a schedule is set to Off (grey). Click Next to move to the next pane. Deploy: To review the results of training before deploying the model, ensure that Deployment type is set to Review first . Click Done to save the algorithm configuration. Now you will notice that the Log anomaly detection-natural language tile, is set to Configured . Now that the Log Anomaly configuration is set, we can start the training. Train & Deploy a Log Anomaly Model From the AI model management page, click on the Manage tab to open the management section. Click the algorithm configuration that you just defined to generate an AI model. The overview page is displayed. In the right sidebar, click Start training . After a few moments, the following response is displayed: Training successfully started . The AI Training tile displays various training status messages: such as Queued . You will see in the Models tile, how the wheel chart turns from red to green as diferent resources are added to the model. Its OK if the anomaly resource name fails to get a model. There are not enough log lines that have information about this resource. Log Anomaly Training will typically take around 30 minutes for this amount of data. This a good time to step away and come back in 30 minutes. Once the training is complete, the Training status tile displays the message: 3 of 3 complete . At this point, the log anomaly models are created. Now the last step in the Log Anomaly Training section is to deploy the model. Click on the Versions tab and confirm there is a model created with Version v1. Click on the 3 dots on the right and select Deploy as shown below. Once the model is deployed, you willsee the Status changed to Deployed as shown below.","title":"Log Anomaly Training"},{"location":"log-anomaly/#log-anomaly-training","text":"","title":"Log Anomaly Training"},{"location":"log-anomaly/#recap","text":"Lets recap what we have done so far: We have defined an integration to a Log Aggregator (EFK) that its receiving logs from an application called qotd and we have pulled logs into CP4WAIOps for a period of 1 day. We have defined an integration to Instana which is observing the environment where qotd is running and we have pulled every resource topology information related to this application into CP4WAIOps. We have enabled one CP4WAIOps policy to create Stories. Now we will do log anomaly training to create a model that CP4WAIOps will use to find anomalies in the logs.","title":"Recap"},{"location":"log-anomaly/#log-anomaly-algorithms","text":"In CP4WAIOps v3.3, two log anomaly detection AI algorithms are available, each of which can run independently of the other. If both algorithms are enabled, then any log anomalies that are discovered by both algorithms are reconciled so that only one alert is generated. The severity of this combined alert is equal to the highest severity of the two alerts. Statistical baseline log anomaly detection extracts specific entities from the logs, such as short text strings that indicate error codes and exceptions. The algorithm combines the entities with other statistics, such as number of logs for each component, and uses the data as a baseline to discover abnormal behavior in your live log data. Natural language log anomaly detection uses natural language techniques on a subset of your log data to discover abnormal behavior. Log anomaly detection takes large amounts of log data and trains on it to learn what is considered normal behavior for a particular component. This model goes beyond just looking at error states or frequency of metadata around log messages. Instead, it determines when something becomes an anomaly compared to what patterns it typically exhibits during normal times.","title":"Log Anomaly Algorithms"},{"location":"log-anomaly/#configuring-log-anomaly-training","text":"From the Home page, under Overview click on AI model management to open the AI Model Management dashboard (feel free to skip the Tour pop-up window). On the AI algorithms tab, on the Log anomaly detection-natural language tile, click Configure . Getting started: Under Data and tool connections, check that at least one connection is listed down the page. Click Next to move to the next pane. Provide details: Here we provide a name and description to identify this algorithm configuration later. For Configuration name, type QOTD-NLP-model . Leave the Configuration description empty. Click Next to move to the next pane. Select data: Specify a date range to train upon. Select Custom and specify the dates listed below. Note that from the full day of application logs that we pulled from the log aggregator, we will select the full 24 hours of data. In a real production deployment, you will tipically pull and later select up to a week of data to have a more representative model. Start Date 05/08/2022 12:00 AM Local Time (UTC -04:00) End Date 05/09/2022 12:00 AM Local Time (UTC -04:00) Click Next to move to the next pane. Filter data: Here we have the option to filter out known anomalies in the training data by specifing date ranges to skip. In this Lab, we don't need to specify any date range to get filtered. Click Next to move to the next pane. Schedule training: Here we can specify a time schedule to run the training if needed. In this Lab we will run the training manually therefore make sure that Run on a schedule is set to Off (grey). Click Next to move to the next pane. Deploy: To review the results of training before deploying the model, ensure that Deployment type is set to Review first . Click Done to save the algorithm configuration. Now you will notice that the Log anomaly detection-natural language tile, is set to Configured . Now that the Log Anomaly configuration is set, we can start the training.","title":"Configuring Log Anomaly Training"},{"location":"log-anomaly/#train-deploy-a-log-anomaly-model","text":"From the AI model management page, click on the Manage tab to open the management section. Click the algorithm configuration that you just defined to generate an AI model. The overview page is displayed. In the right sidebar, click Start training . After a few moments, the following response is displayed: Training successfully started . The AI Training tile displays various training status messages: such as Queued . You will see in the Models tile, how the wheel chart turns from red to green as diferent resources are added to the model. Its OK if the anomaly resource name fails to get a model. There are not enough log lines that have information about this resource. Log Anomaly Training will typically take around 30 minutes for this amount of data. This a good time to step away and come back in 30 minutes. Once the training is complete, the Training status tile displays the message: 3 of 3 complete . At this point, the log anomaly models are created. Now the last step in the Log Anomaly Training section is to deploy the model. Click on the Versions tab and confirm there is a model created with Version v1. Click on the 3 dots on the right and select Deploy as shown below. Once the model is deployed, you willsee the Status changed to Deployed as shown below.","title":"Train &amp; Deploy a Log Anomaly Model"}]}