{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"about/","text":"Lab: Log Anomaly + Instana topology Author: Jorge Gonzalez Orozco Date: May 2022 \u00a9 Copyright IBM Corp. 2022","title":"About"},{"location":"inference/","text":"Inference After your trained model is successfully deployed, update your Log Ops integration, such as ELK to start log anomaly detection: Choose Live data for continuous AI training and anomaly detection as the option for training mode. Enable Data flow . Data flow when your trained AI models are deployed, data flows through the cp4waiops-cartridge-windowed-logs-1000-1000 topic. Any detected anomalies, such as log anomalies, Netcool Operations Insights events, flow through the cp4waiops-cartridge.lifecycle.input.events topic. The Event Grouping Service then consumes and groups the events from the cp4waiops-cartridge.lifecycle.input.events topic, and resolves the entities by using the Topology Service. The service gets the localization and blast radius from the localization and blast radius service, and finally produces stories to the cp4waiops-cartridge.lifecycle.input.stories topic. These stories are eventually surfaced as notifications on the reactive Slack channel.","title":"Inference Results"},{"location":"inference/#inference","text":"After your trained model is successfully deployed, update your Log Ops integration, such as ELK to start log anomaly detection: Choose Live data for continuous AI training and anomaly detection as the option for training mode. Enable Data flow .","title":"Inference"},{"location":"inference/#data-flow","text":"when your trained AI models are deployed, data flows through the cp4waiops-cartridge-windowed-logs-1000-1000 topic. Any detected anomalies, such as log anomalies, Netcool Operations Insights events, flow through the cp4waiops-cartridge.lifecycle.input.events topic. The Event Grouping Service then consumes and groups the events from the cp4waiops-cartridge.lifecycle.input.events topic, and resolves the entities by using the Topology Service. The service gets the localization and blast radius from the localization and blast radius service, and finally produces stories to the cp4waiops-cartridge.lifecycle.input.stories topic. These stories are eventually surfaced as notifications on the reactive Slack channel.","title":"Data flow"},{"location":"integration/","text":"Welcome to the Cloud Pack for Watson AIOps v3.3 Lab For full product documentation visit this link . Lab Overview >>> TBD >>> Add Lab overview steps. Lets start by connecting to the Cloud Pack for Watson AIOps (CP4WAIOps) console. Get the URL and login credentials from the Lab Parameters Table . Define the EFK Integration EFK is a variant of ELK (Elasticsearch, Logstash, and Kibana). EFK is a suite of tools combining Elasticsearch, Fluentd, and Kibana that functions as a log aggregation tool. To simplify the installation and configuration effort for installing the different components of the EFK stack on OpenShift, we leverage the OpenShift Logging library from OpenShift. OpenShift customers that prefer not to spend part of their budget on a commercial log aggregator such as Humio, Splunk, or LogDNA, more than likely use the OpenShift Logging library. To have AI Manager collect logs from the EFK installation that leverages the OpenShift Logging library, you need to define an EFK integration. The lab will provide the values that you should use for defining the EFK integration. From the Home page, under Overview clik on Data and tool connections on the left side of the page. Click on the Add connection button on the top right. On the ELK card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Complete the ELK, Add connection form, with the following values: Name : Name of the ELK integration, for example EFK for QOTD . ELK service URL : Get the service URL for the EFK installation from the Lab Parameters Table . Kibana URL : Get URL for Kibana from the Lab Parameters Table . Authentication type : Set this value to Token . Token : Get the token from the Lab Parameters Table . Kibana port : Use 443 for this field. Click on the Next button. Field mapping : Use the mapping shown below instead of the default mapping provided on the ELK integration. Make sure you see the Valid JSON configuration message after that: { \"codec\": \"elk\", \"message_field\": \"message\", \"log_entity_types\": \"kubernetes.container_image_id, kubernetes.host, kubernetes.pod_name, kubernetes.namespace_name\", \"instance_id_field\": \"kubernetes.container_name\", \"rolling_time\": 10, \"timestamp_field\": \"@timestamp\" } Click on the Test Connection button and confirm you get Test Succeded Click on the Next button. Data flow : Turn this on. We will ingest historical data that we will use for training. Mode : Select the Historical data for initial AI training option. Start date: April 23, 2022 End Date: April 27, 2022 The following screens show the form update flow (note that config values may be different in the screen) Finally, click on the Done button. After some time, you will see the message Connection completed. IBM Cloud Pak for Watson AIOps has successfully processed your request >>> TBD >>> Add step to verify completion of 1 day of log data load Define the Instana Integration The CP4WAIOps will consume topology information from Instana therefore we will configure the integration between the CP4WAIOps and Instana. Lets verify first that there is no topology data in the system. From the Home page, clik on Resource management under Overview on the left side of the page. On the Resource management page, make sure there are no Applications, Resource groups nor Resources defined, as shown below >>> TBD >>> Replace screen shot Now, lets define the Instana integration. From the Home page, clik on Data and tool connections under Overview on the left side of the page. Click on the Add connection button on the top right. On the Instana card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Enter the following Add connection information: Name : The display name of your connection, for example Instana connection . Description : An optional description for the connection. Endpoint : Get the URL for the Instana Endpoint from the Lab Parameters Table . API Token : Get the API Token from the Lab Parameters Table . Deployment options : Select local (Note that it is possible to deploy an Instana conection gateway remotely, but in this lab we will deploy in the same local cluster). Click Next. Enter the following Collect topology data : Enable data flow : Set this toggle button to on (green). Time window (seconds): Leave as it is. This is the windowSize within the Instana API. Connection Intervals (seconds): Leave as it is. This is how frequently to run the job to collect topology. Application name allowlist pattern : This allows to select from the set of applications that Instana is \"observing\" which one we will pull data from. In this Lab, we will pull topology data from a single application called qotd. Type in qotd . Import Instana application perspectives as Cloud Pak for Watson AIOps applications : Make sure this toggle button is set to on (green). This option will save us some time as we don't need to manually create in CP4WAIOps an application that group the topology resources that we pull from Instana. In CP4WAIOps, an application represents a group of resources put together. Click Next. Enter the following in Collect event data : Enable data flow : Slide this toggle button to off (grey). We will not collect event data in this Lab. Click Next. Enter the following in Collect metric data : Enable data flow : Slide this toggle button to off (grey). We will not collect metric data in this Lab. Click Done. Now, lets verify that CP4WAIOps actually received topology data. From the Home page, clik on Resource management under Overview on the left side of the page. On the Resource management page, you will see a new application defined called qotd as shown below: Click on the application qotd and you will see the topology resources related to this application as shown in the picture below. Feel free to zoom-in to see details.","title":"Start Here"},{"location":"integration/#welcome-to-the-cloud-pack-for-watson-aiops-v33-lab","text":"For full product documentation visit this link .","title":"Welcome to the Cloud Pack for Watson AIOps v3.3 Lab"},{"location":"integration/#lab-overview","text":">>> TBD >>> Add Lab overview steps. Lets start by connecting to the Cloud Pack for Watson AIOps (CP4WAIOps) console. Get the URL and login credentials from the Lab Parameters Table .","title":"Lab Overview"},{"location":"integration/#define-the-efk-integration","text":"EFK is a variant of ELK (Elasticsearch, Logstash, and Kibana). EFK is a suite of tools combining Elasticsearch, Fluentd, and Kibana that functions as a log aggregation tool. To simplify the installation and configuration effort for installing the different components of the EFK stack on OpenShift, we leverage the OpenShift Logging library from OpenShift. OpenShift customers that prefer not to spend part of their budget on a commercial log aggregator such as Humio, Splunk, or LogDNA, more than likely use the OpenShift Logging library. To have AI Manager collect logs from the EFK installation that leverages the OpenShift Logging library, you need to define an EFK integration. The lab will provide the values that you should use for defining the EFK integration. From the Home page, under Overview clik on Data and tool connections on the left side of the page. Click on the Add connection button on the top right. On the ELK card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Complete the ELK, Add connection form, with the following values: Name : Name of the ELK integration, for example EFK for QOTD . ELK service URL : Get the service URL for the EFK installation from the Lab Parameters Table . Kibana URL : Get URL for Kibana from the Lab Parameters Table . Authentication type : Set this value to Token . Token : Get the token from the Lab Parameters Table . Kibana port : Use 443 for this field. Click on the Next button. Field mapping : Use the mapping shown below instead of the default mapping provided on the ELK integration. Make sure you see the Valid JSON configuration message after that: { \"codec\": \"elk\", \"message_field\": \"message\", \"log_entity_types\": \"kubernetes.container_image_id, kubernetes.host, kubernetes.pod_name, kubernetes.namespace_name\", \"instance_id_field\": \"kubernetes.container_name\", \"rolling_time\": 10, \"timestamp_field\": \"@timestamp\" } Click on the Test Connection button and confirm you get Test Succeded Click on the Next button. Data flow : Turn this on. We will ingest historical data that we will use for training. Mode : Select the Historical data for initial AI training option. Start date: April 23, 2022 End Date: April 27, 2022 The following screens show the form update flow (note that config values may be different in the screen) Finally, click on the Done button. After some time, you will see the message Connection completed. IBM Cloud Pak for Watson AIOps has successfully processed your request >>> TBD >>> Add step to verify completion of 1 day of log data load","title":"Define the EFK Integration"},{"location":"integration/#define-the-instana-integration","text":"The CP4WAIOps will consume topology information from Instana therefore we will configure the integration between the CP4WAIOps and Instana. Lets verify first that there is no topology data in the system. From the Home page, clik on Resource management under Overview on the left side of the page. On the Resource management page, make sure there are no Applications, Resource groups nor Resources defined, as shown below >>> TBD >>> Replace screen shot Now, lets define the Instana integration. From the Home page, clik on Data and tool connections under Overview on the left side of the page. Click on the Add connection button on the top right. On the Instana card, select Add connection . Take a moment to read the connection overview on the right side slider, then click on Connect , as shown in the following screen. Enter the following Add connection information: Name : The display name of your connection, for example Instana connection . Description : An optional description for the connection. Endpoint : Get the URL for the Instana Endpoint from the Lab Parameters Table . API Token : Get the API Token from the Lab Parameters Table . Deployment options : Select local (Note that it is possible to deploy an Instana conection gateway remotely, but in this lab we will deploy in the same local cluster). Click Next. Enter the following Collect topology data : Enable data flow : Set this toggle button to on (green). Time window (seconds): Leave as it is. This is the windowSize within the Instana API. Connection Intervals (seconds): Leave as it is. This is how frequently to run the job to collect topology. Application name allowlist pattern : This allows to select from the set of applications that Instana is \"observing\" which one we will pull data from. In this Lab, we will pull topology data from a single application called qotd. Type in qotd . Import Instana application perspectives as Cloud Pak for Watson AIOps applications : Make sure this toggle button is set to on (green). This option will save us some time as we don't need to manually create in CP4WAIOps an application that group the topology resources that we pull from Instana. In CP4WAIOps, an application represents a group of resources put together. Click Next. Enter the following in Collect event data : Enable data flow : Slide this toggle button to off (grey). We will not collect event data in this Lab. Click Next. Enter the following in Collect metric data : Enable data flow : Slide this toggle button to off (grey). We will not collect metric data in this Lab. Click Done. Now, lets verify that CP4WAIOps actually received topology data. From the Home page, clik on Resource management under Overview on the left side of the page. On the Resource management page, you will see a new application defined called qotd as shown below: Click on the application qotd and you will see the topology resources related to this application as shown in the picture below. Feel free to zoom-in to see details.","title":"Define the Instana Integration"},{"location":"log-anomaly/","text":"Log Anomaly Training Recap Lets recap what we have done so far: We have defined an integration to a Log Aggregator (EFK) that its receiving logs from an application called qotd and we have pulled logs into CP4WAIOps for a period of 1 day. We have defined an integration to Instana which is observing the environment where qotd is running and we have pulled every resource topology information related to this application into CP4WAIOps. Now we will do log anomaly training to create a model that CP4WAIOps will use to find anomalies in the logs. Log Anomaly Algorithms In CP4WAIOps v3.3, two log anomaly detection AI algorithms are available, each of which can run independently of the other. If both algorithms are enabled, then any log anomalies that are discovered by both algorithms are reconciled so that only one alert is generated. The severity of this combined alert is equal to the highest severity of the two alerts. Statistical baseline log anomaly detection extracts specific entities from the logs, such as short text strings that indicate error codes and exceptions. The algorithm combines the entities with other statistics, such as number of logs for each component, and uses the data as a baseline to discover abnormal behavior in your live log data. Natural language log anomaly detection uses natural language techniques on a subset of your log data to discover abnormal behavior. Log anomaly detection takes large amounts of log data and trains on it to learn what is considered normal behavior for a particular component. This model goes beyond just looking at error states or frequency of metadata around log messages. Instead, it determines when something becomes an anomaly compared to what patterns it typically exhibits during normal times. Configuring Log Anomaly Training From the Home page, under Overview click on AI model management to open the AI Model Management dashboard (feel free to skip the Tour pop-up window). On the AI algorithms tab, on the Log anomaly detection-natural language tile, click Configure . Getting started: Under Data and tool connections, check that at least one connection is listed down the page. Click Next to move to the next pane. Provide details: Here we provide a name and description to identify this algorithm configuration later. Lets leave the default Configuration name. Click Next to move to the next pane. Select data: Specify a date range to train upon. Select Custom and specify Date1 as Start Date and Date2 as End Date. Click Next to move to the next pane. Filter data: Here we have the option to filter out known anomalies in the training data by specifing date ranges to skip. In this Lab, we don't need to specify any date range to get filtered. Click Next to move to the next pane. Schedule training: Here we can specify a time schedule to run the training if needed. In this Lab we will run the training manually therefore make sure that Run on a schedule is set to Off (grey). Click Next to move to the next pane. Deploy: To review the results of training before deploying the model, ensure that Deployment type is set to Review first . Click Done to save the algorithm configuration. Now you will notice that the Log anomaly detection-natural language tile, is set to Configured . Now that the Log Anomaly configuration is set, we can start the training. Start Training From the AI model management page, click on the Manage tab to open the management section. Click the algorithm configuration that you just defined to generate an AI model. The overview page is displayed. In the right sidebar, click Start training . After a few moments, the following response is displayed: Training successfully started . The AI Training tile displays the message: Data retrieving. Training is now in progress. Once the training is complete, the AI Training tile displays the message: Training complete . At this point, AI models are created, with a specified data quality. To deploy the model, click the algorithm configuration for whose model that you want to deploy. The overview page is displayed. In the right sidebar, click Deploy vX , where X is the version number of the model that is generated, but not deployed. Once the model is deployed, you can check the status of the model under the Versions tab.","title":"Log Anomaly Training"},{"location":"log-anomaly/#log-anomaly-training","text":"","title":"Log Anomaly Training"},{"location":"log-anomaly/#recap","text":"Lets recap what we have done so far: We have defined an integration to a Log Aggregator (EFK) that its receiving logs from an application called qotd and we have pulled logs into CP4WAIOps for a period of 1 day. We have defined an integration to Instana which is observing the environment where qotd is running and we have pulled every resource topology information related to this application into CP4WAIOps. Now we will do log anomaly training to create a model that CP4WAIOps will use to find anomalies in the logs.","title":"Recap"},{"location":"log-anomaly/#log-anomaly-algorithms","text":"In CP4WAIOps v3.3, two log anomaly detection AI algorithms are available, each of which can run independently of the other. If both algorithms are enabled, then any log anomalies that are discovered by both algorithms are reconciled so that only one alert is generated. The severity of this combined alert is equal to the highest severity of the two alerts. Statistical baseline log anomaly detection extracts specific entities from the logs, such as short text strings that indicate error codes and exceptions. The algorithm combines the entities with other statistics, such as number of logs for each component, and uses the data as a baseline to discover abnormal behavior in your live log data. Natural language log anomaly detection uses natural language techniques on a subset of your log data to discover abnormal behavior. Log anomaly detection takes large amounts of log data and trains on it to learn what is considered normal behavior for a particular component. This model goes beyond just looking at error states or frequency of metadata around log messages. Instead, it determines when something becomes an anomaly compared to what patterns it typically exhibits during normal times.","title":"Log Anomaly Algorithms"},{"location":"log-anomaly/#configuring-log-anomaly-training","text":"From the Home page, under Overview click on AI model management to open the AI Model Management dashboard (feel free to skip the Tour pop-up window). On the AI algorithms tab, on the Log anomaly detection-natural language tile, click Configure . Getting started: Under Data and tool connections, check that at least one connection is listed down the page. Click Next to move to the next pane. Provide details: Here we provide a name and description to identify this algorithm configuration later. Lets leave the default Configuration name. Click Next to move to the next pane. Select data: Specify a date range to train upon. Select Custom and specify Date1 as Start Date and Date2 as End Date. Click Next to move to the next pane. Filter data: Here we have the option to filter out known anomalies in the training data by specifing date ranges to skip. In this Lab, we don't need to specify any date range to get filtered. Click Next to move to the next pane. Schedule training: Here we can specify a time schedule to run the training if needed. In this Lab we will run the training manually therefore make sure that Run on a schedule is set to Off (grey). Click Next to move to the next pane. Deploy: To review the results of training before deploying the model, ensure that Deployment type is set to Review first . Click Done to save the algorithm configuration. Now you will notice that the Log anomaly detection-natural language tile, is set to Configured . Now that the Log Anomaly configuration is set, we can start the training.","title":"Configuring Log Anomaly Training"},{"location":"log-anomaly/#start-training","text":"From the AI model management page, click on the Manage tab to open the management section. Click the algorithm configuration that you just defined to generate an AI model. The overview page is displayed. In the right sidebar, click Start training . After a few moments, the following response is displayed: Training successfully started . The AI Training tile displays the message: Data retrieving. Training is now in progress. Once the training is complete, the AI Training tile displays the message: Training complete . At this point, AI models are created, with a specified data quality. To deploy the model, click the algorithm configuration for whose model that you want to deploy. The overview page is displayed. In the right sidebar, click Deploy vX , where X is the version number of the model that is generated, but not deployed. Once the model is deployed, you can check the status of the model under the Versions tab.","title":"Start Training"}]}